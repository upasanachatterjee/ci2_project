{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ds_utils import load_dataset_from_huggingface\n",
    "ds = load_dataset_from_huggingface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.filter(lambda example: len(\"\".join(example[\"text\"]).split()) > 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds.select_columns(['id','text']).to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def create_sample_file(word_count, df, output_file):\n",
    "    tasks = []\n",
    "    for index, row in df.iterrows():\n",
    "        article = row['text']\n",
    "        task = {\n",
    "            \"custom_id\": row['id'],  # custom_id must be a string\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": \"gpt-4o-mini\",\n",
    "                \"temperature\": 0.1,\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are a helpful assistant tasked with creating summaries of user given text. These summaries preserve the tone, voice and perspective of the original text.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"\"\"Generate a list of at most {word_count} topics \n",
    "                                        for the following article. Keep the authorial voice, \n",
    "                                        perspective, and tone. \\n\\n\\n {article}\"\"\"\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        }\n",
    "        tasks.append(task)\n",
    "\n",
    "    for i in range(0,len(tasks), 200):\n",
    "        with open(f\"samples/{output_file}_{i}.jsonl\", 'w') as file:\n",
    "            for j in range(i, min(len(tasks), i + 200)):\n",
    "                file.write(json.dumps(tasks[j]) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import time\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"openai\"))\n",
    "\n",
    "def upload(file):\n",
    "    # with open(\"test.jsonl\", \"w\") as f:\n",
    "    #     for elt in lst:\n",
    "    #         f.write(f\"{json.dumps(elt)}\\n\")\n",
    "\n",
    "    # Upload to OpenAI file API\n",
    "    batch_file = client.files.create(\n",
    "        file=open(file, \"rb\"),\n",
    "        purpose=\"batch\"\n",
    "    )\n",
    "\n",
    "    # Start batch job\n",
    "    batch_job = client.batches.create(\n",
    "        input_file_id=batch_file.id,\n",
    "        endpoint=\"/v1/responses\", #\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\"\n",
    "    )\n",
    "    \n",
    "    # with open(\"batch_ids.csv\", \"a\") as f:\n",
    "    #     f.write(f\"{batch_job.id}\\n\")\n",
    "\n",
    "    time.sleep(10)\n",
    "    retrieved = client.batches.retrieve(batch_id=batch_job.id)\n",
    "    print(retrieved.status)\n",
    "\n",
    "    if retrieved.status != 'failed':\n",
    "        with open(\"uploaded.csv\", \"a\") as f:\n",
    "            f.write(f\"{file},{batch_file.id},{batch_job.id}\\n\")\n",
    "        return True\n",
    "    else:\n",
    "        print(retrieved)\n",
    "        return False\n",
    "    \n",
    "    # if status == 'failed':\n",
    "    #     return False\n",
    "\n",
    "    # return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_samples(df, word_count):\n",
    "    tasks = []\n",
    "    for index, row in df.iterrows():\n",
    "        id = row['id']\n",
    "        article = row['text']\n",
    "        task = {\n",
    "            \"custom_id\": f\"{id}_{word_count}\",  # custom_id must be a string\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/responses\",\n",
    "            \"body\": {\n",
    "                \"model\": \"gpt-4o-mini\",\n",
    "                \"input\" : f\"Generate an exactly {word_count} word summary of the following article. Keep the authorial voice, perspective, and tone. Write from the perspective of the original article, not as a third party summarizing the article. \\n {article}\",\n",
    "                \"text\": {\n",
    "                \"format\" : {\n",
    "                    \"name\": \"response_type\",\n",
    "                    \"schema\": {\n",
    "                    \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"article\": {\n",
    "                        \"type\": \"string\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"article\"],\n",
    "                    \"additionalProperties\": False\n",
    "                    },\n",
    "                    \"type\": \"json_schema\" \n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        tasks.append(task)\n",
    "\n",
    "\n",
    "    for i in range(0,len(tasks), 200):\n",
    "        with open(f\"samples/{word_count}_{i}.jsonl\", 'w') as f:\n",
    "            for j in range(i, min(len(tasks), i + 200)):\n",
    "                f.write(f\"{json.dumps(tasks[j])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded = []\n",
    "processed_batches = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "\n",
    "files_to_see = True\n",
    "\n",
    "while files_to_see:\n",
    "    files = glob.glob(\"samples/*.jsonl\")\n",
    "    files.sort()\n",
    "\n",
    "    files = [x for x in files if x not in uploaded]\n",
    "\n",
    "    if len(files) < 1:\n",
    "        files_to_see = False\n",
    "        break\n",
    "\n",
    "    for f in files[:5]:\n",
    "        success = upload(f)\n",
    "        if success:\n",
    "            uploaded.append(f)\n",
    "\n",
    "    time.sleep(60*60)\n",
    "\n",
    "    lst = client.batches.list()\n",
    "    completed = []\n",
    "    for elt in lst.data:\n",
    "        if elt.status == \"completed\":\n",
    "            completed.append(elt.id)\n",
    "\n",
    "    new_batches = [x for x in completed if x not in processed_batches]\n",
    "    for batch in new_batches:\n",
    "        batch_job = client.batches.retrieve(batch)\n",
    "        result_file_id = batch_job.output_file_id\n",
    "        results_list = []\n",
    "        result = client.files.content(result_file_id).content\n",
    "        result = result.decode('utf-8')\n",
    "        result_entries = result.strip().split(\"\\n\")\n",
    "        for r in result_entries:\n",
    "            results_list.append(json.loads(r))\n",
    "\n",
    "        custom_id = results_list[0]['custom_id'].split(\"_\")\n",
    "        if len(custom_id) == 2 and custom_id[1] == '5':\n",
    "            for item in results_list:\n",
    "                idx = item['custom_id'].split(\"_\")[0]\n",
    "                summary = item['response']['body']['output'][0]['content'][0]['text']\n",
    "                with open(\"summary_5.psv\", \"a\") as f:\n",
    "                    f.write(f\"{idx}|{json.loads(summary)['article']}\\n\")\n",
    "\n",
    "        processed_batches.append(batch)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
